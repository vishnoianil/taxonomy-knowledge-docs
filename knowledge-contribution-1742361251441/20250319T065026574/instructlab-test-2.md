Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
Model Context Protocol
Brief description of Idea
MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources.

Long description of idea(Technical details)
The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

Product(s) involved
RHEL AI, OpenShift AI

Existing assets

https://x.com/alexalbert__/status/1861079874385203522
https://github.com/modelcontextprotocol/servers
https://modelcontextprotocol.io/quickstart


Agentic framework
RH needs an Agentic framework story we have been dancing around it for the last year.
Brief description of idea
Low code agent framework
Long description of idea(Technical details)
We have seen some success with the UI capabilities of OpenShift. We could provide a similar experience for Agents to actually do things to have better results, more customized results, and the possibility to execute actions based on the findings. Joe mentioned during the 2025 AI call that Agents can be the way we tie predictive and Gen AI together.
Product(s) involved
Instructlab,RHOAI,OpenShift
Existing assets(optional)
https://github.com/ragapp/ragapp


Networking Guidance for Distributed training
Brief description of idea

Distributed training is possible within RHOAI but we are not providing any guidance on what network changes need to occur to successfully accomplish this.



Long description of idea(Technical details)


Product(s) involved
RHOAI,OpenShift
Existing assets(optional)


AI Model Pipeline
Brief description of idea
There are a ton of missing features around packaging a model in OCI and signing the model and the OCI artifact. OCI is going to change the way RHOAI operates. We should put the fixes in place.
Long description of idea(Technical details)
Currently, the pipeline service uses KFP which is based on Argo Workflows. We do not have the capabilities that we productized with tekton and Sigstore to sign and attest these artifacts. We should also look to enforce this from the model serving standpoint.
Product(s) involved

https://github.com/argoproj/argo-workflows/pull/9837
Existing assets(optional)


KitOps

Brief description of idea
KitOps is one of the early projects for AI/ML ops management that focuses on packaging all project artifacts (code, datasets, models, and documentation) into versioned, immutable ModelKits, ensuring reproducibility, collaboration, and compliance across teams.... It integrates with existing tools, reducing workflow friction and enabling secure, structured deployment processes. They are on the path to become/submit to become an industry standard. It would be great if we could have our foot in the door here. We should also see how we can integrate the use cases covered by KitOps into RHOAI, especially into the newly introduced Model registry in RHOAI.

Long description of idea(Technical details)
We should keep an eye on KitOps and how we can tie it to Ramalama, Podman and other projects in the field that already participate in the problem space and partially solve some of the issues here. KitOps can bring additional enhancements in following areas:

Compliance and Auditability - KitOps ensures traceability by recording the provenance of datasets, models, and code. It stores these packages in a secure, standards-based format (OCI-compliant), which aligns with organizational policies on security and version control. Ties to AIBOM discussions. KitOps definitely offers the most complex dependency graph to potentially generate an AIBOM compared to all the tools out there today
Enhanced Reproducibility - with a Kitfile, you can describe your project assets in detail, ensuring anyone can recreate the same environment or experiment. This is essential for both scientific reproducibility and consistent deployment in production
Centralized Project Management - KitOps consolidates all elements of your AI/ML project—code, datasets, models, and documentation—into a single, versioned, immutable package called a ModelKit. This simplifies tracking which components (e.g., specific dataset versions or code bases) were used together, reducing confusion and errors in complex projects

Product(s) involved
Instructlab, RHOAI, OpenShift, RHELAI, Ramalama, Podman, Podman Desktop
Existing assets(optional)
Links:
https://kitops.ml/docs/overview.html
How to deploy a ModelKit from Jozu Hub to Openshift #openshift #machinelearning
https://github.com/cncf/sandbox/issues/313
https://developers.redhat.com/articles/2024/09/16/enhance-llms-instructlab-kitops#add_new_knowledge_to_the_llm
Related projects
https://github.com/containers/ramalama
https://github.com/opendatahub-io/model-registry
https://github.com/CloudNativeAI/model-spec

AI Observability
Brief description of idea

Create PoCs for data observability in RH AI while at the same time proving how opentelemetry-collector can unify data collection across products, platforms, and footprints.
Long description of idea(Technical details)
Red Hat AI needs a data observability solution as well as a dedicated solution for tracking hardware accelerator utilization. There are a handful of open source projects we should investigate and create PoCs around. In addition, in a meeting last week (Sean Cohen, KB, IBM and a few others) IBM suggested the use of Databand with RHOAI. The caveat is that this is not open source, so it’s a non-starter if they don’t plan on open-sourcing it. If they do plan on open-sourcing, it might fit in well with RH AI.

Along with PoCs for data observability, with opentelemetry-collector now in RHEL 9.5, such PoCs will also demonstrate how this tool can integrate across RH portfolio to provide unified data collection that can be exported to and visualized in OpenShift.

A few related projects:
https://github.com/openlit/openlit/tree/main/otel-gpu-collector
https://opentelemetry.io/docs/specs/semconv/system/hardware-metrics/#hwgpu---gpu-metrics
https://docs.splunk.com/observability/en/gdi/opentelemetry/components/apache-spark-receiver.html
https://github.com/MarquezProject/marquez
https://github.com/traceloop/openllmetry
Product(s) involved

RHEL AI and OpenShift AI
Existing assets(optional)




K8sGPT
Brief description of idea
Explore and Integrate the K8sGPT cluster analysis tool into an OpenShift Cluster
Long description of idea(Technical details)
K8sGPT is a CNCF sandbox project to make monitoring and diagnosing issues with Kubernetes Clusters simpler by adding an LLM interface.

“K8sGPT is a tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. It has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.” - https://k8sgpt.ai/

Making this available for OpenShift users seems like a great addition to the product. In addition, this project currently defaults to using OpenAi for its model server. Our existing tool sets and knowhow for custom model serving could make a good addition to this project for Enterprise users.
Product(s) involved
OpenShift, OpenShift LightSpeed (maybe?)
Existing assets(optional)
https://k8sgpt.ai/

AI SDK
Brief description of idea
An SDK to help build and deploy "cloud native" AI Models from the start, rather than trying to containerise bare metal models retrospectively.
Long description of idea(Technical details)
The AI SDK provides the tools to build, test, profile, package, deploy and monitor AI models. Initially, the SDK can be used to help “cloudify” the bare metal models that exist today. Over time, the SDK can allow ML engineers to make models truly behave like cloud services. Leading practices and code patterns that are shared across Models are included in the SDK to help prevent reinventing the wheel.

Product(s) involved
RHOAI,OpenShift
Existing assets(optional)
Operator SDK: https://sdk.operatorframework.io/



Developer Hub templates
Brief description of idea
Generate templates creating a tight integration between Developer Hub and RHOAI
Long description of idea(Technical details)
Right now RHOAI is somewhat of a choose your own adventure solution the Developer Hub templates would hope to add some smoothness to common processes within RHOAI. This can also be an automation point for Instructlab.
Product(s) involved
RHOAI,Developer Hub
Existing assets(optional)



Building and orchestrating synthetic playgrounds for training LLM-based agents
Brief description of idea
An OpenShift+Ansible-based solution for creating custom ad-hoc simulation environments (“playgrounds”) as an enabler for production-grade agent training.
Long description of idea(Technical details)
As opposed to regular LLMs, agents are capable of making decisions and performing actions. Typically, training an agent involves fine-tuning a pretrained LLM via some kind of on-policy reinforcement learning, where an agent creates a plan, executes it using the available tools, observes the result and receives a reward. Agents are optimized to produce good results on the supported tasks by performing millions of attempts at performing these tasks. These attempts mostly fail in the beginning and the quality slowly improves as the training progresses.

For the training procedure to be possible, it is imperative to set up a virtual, isolated "playground", that is, a simulation environment where an agent can go through millions of iterations until it stabilizes and achieves a high success rate on the target tasks. For example, training a desktop assistant requires a mockup of a machine it is intended to be installed on (e.g., a virtual machine), while for a web agent one has to simulate different websites / web services. More complicated agentic and multi-agentic systems necessitate even more complicated simulation setup.

As an industry leader in containerization and infrastructure automation, and with products such as OpenShift and Ansible, Red Hat is uniquely positioned to become a driving force in this space. Thus, we propose to explore the potential for developing a solution for creating simulation environments (“playgrounds”) as an enabler for production-grade agent training. To the best of our knowledge, no existing tool provides this capability today.
Product(s) involved
RHOAI,OpenShift, Ansible
Existing assets(optional)
